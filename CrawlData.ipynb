{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLMOVqgukUfO",
        "outputId": "8f49e250-1fe7-4755-d96f-59a699dbad1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (0.3.47)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (0.3.18)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2->langchain-community) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: unstructured in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (0.17.2)\n",
            "Requirement already satisfied: chardet in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (5.3.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (3.12.2)\n",
            "Requirement already satisfied: backoff in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (0.31.3)\n",
            "Requirement already satisfied: wrapt in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (7.0.0)\n",
            "Requirement already satisfied: python-oxmsg in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (0.0.2)\n",
            "Requirement already satisfied: html5lib in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk->unstructured) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
            "Requirement already satisfied: olefile in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->unstructured) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->unstructured) (2024.12.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (44.0.2)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (2.10.6)\n",
            "Requirement already satisfied: pypdf>=4.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (5.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from unstructured-client->unstructured) (0.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: pymongo in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (4.11.3)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.48.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: rank-bm25 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from rank-bm25) (1.26.4)\n",
            "Requirement already satisfied: langchain-ollama in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (0.3.0)\n",
            "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-ollama) (0.3.47)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.10.6)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-ollama) (1.26.20)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt các thư viện cần thiết\n",
        "!pip install -U langchain-community\n",
        "!pip install unstructured\n",
        "!pip install pymongo\n",
        "!pip install sentence-transformers\n",
        "!pip install beautifulsoup4\n",
        "!pip install rank-bm25\n",
        "!pip install langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def advanced_clean_text(raw_html: str) -> str:\n",
        "    \"\"\"\n",
        "    Làm sạch HTML, sau đó loại bỏ các ký tự Unicode control, zero-width, BOM,\n",
        "    và các ký tự lạ không nằm trong bảng chữ cái tiếng Việt mở rộng.\n",
        "    \"\"\"\n",
        "    # 1) Dùng BeautifulSoup để loại bỏ thẻ HTML, lấy text thuần\n",
        "    text = BeautifulSoup(raw_html, \"html.parser\").get_text(separator=\" \", strip=True)\n",
        "    \n",
        "    # 2) Loại bỏ các ký tự control (ASCII 0-31, 127-159)\n",
        "    text = re.sub(r'[\\u0000-\\u001F\\u007F-\\u009F]', ' ', text)\n",
        "    \n",
        "    # 3) Loại bỏ các ký tự zero-width (vd: \\u200B, \\u200C, \\u200D, \\uFEFF)\n",
        "    text = re.sub(r'[\\u200B-\\u200F\\uFEFF]', '', text)\n",
        "    \n",
        "    # 4) Loại bỏ các ký tự BOM (Byte Order Mark)\n",
        "    text = re.sub(r'\\ufeff', '', text)\n",
        "    \n",
        "    # 5) Tuỳ chọn: Loại bỏ các ký tự “ngoài vùng” (chỉ giữ lại: \n",
        "    #    - ASCII cơ bản\n",
        "    #    - các ký tự tiếng Việt có dấu (U+00C0–U+024F, U+1E00–U+1EFF)\n",
        "    #    - các dấu câu cơ bản . , ; : - ? ! ( ) / v.v.\n",
        "    #    - khoảng trắng\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u024F\\u1E00-\\u1EFF\\s\\.\\,\\;\\:\\-\\?\\!\\(\\)/…]', '', text)\n",
        "    \n",
        "    # 6) Xoá khoảng trắng thừa\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def advanced_clean_text(raw_html: str) -> str:\n",
        "    \"\"\"\n",
        "    Làm sạch HTML và loại bỏ:\n",
        "      - Các ký tự control (ASCII 0-31, 127-159)\n",
        "      - Các ký tự zero-width (ví dụ: \\u200B, \\u200C, \\u200D, \\uFEFF)\n",
        "      - Ký tự BOM (\\ufeff)\n",
        "      - Các ký tự không mong muốn (chỉ giữ lại chữ, số, khoảng trắng và một số dấu câu cơ bản)\n",
        "    \"\"\"\n",
        "    # 1) Loại bỏ thẻ HTML bằng BeautifulSoup\n",
        "    text = BeautifulSoup(raw_html, \"html.parser\").get_text(separator=\" \", strip=True)\n",
        "    \n",
        "    # 2) Loại bỏ các ký tự điều khiển (control characters)\n",
        "    text = re.sub(r'[\\u0000-\\u001F\\u007F-\\u009F]', ' ', text)\n",
        "    \n",
        "    # 3) Loại bỏ các ký tự zero-width và BOM\n",
        "    text = re.sub(r'[\\u200B-\\u200F\\uFEFF]', '', text)\n",
        "    text = re.sub(r'\\ufeff', '', text)\n",
        "    \n",
        "    # 4) Loại bỏ các ký tự không mong muốn, giữ lại chữ cái (cả tiếng Việt), số, khoảng trắng và dấu câu cơ bản\n",
        "    text = re.sub(r'[^a-zA-Z0-9À-ỹà-ỹ\\s\\.\\,\\;\\:\\-\\?\\!\\(\\)]', ' ', text)\n",
        "    \n",
        "    # 5) Loại bỏ khoảng trắng thừa\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# --- Khởi tạo PhoBERT ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\", use_fast=False)\n",
        "model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def get_phobert_embedding(text):\n",
        "    \"\"\"\n",
        "    Tính embedding với PhoBERT, áp dụng advanced_clean_text để giảm lỗi \"index out of range\".\n",
        "    \"\"\"\n",
        "    # Bước 1: Làm sạch nâng cao\n",
        "    cleaned_text = advanced_clean_text(text)\n",
        "    \n",
        "    # Nếu sau khi làm sạch mà chuỗi rỗng thì bỏ qua\n",
        "    if not cleaned_text.strip():\n",
        "        raise ValueError(\"Text rỗng sau khi làm sạch!\")\n",
        "    \n",
        "    try:\n",
        "        # 2) Tokenize với PhoBERT\n",
        "        inputs = tokenizer(\n",
        "            cleaned_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # 3) Lấy output, mean pooling, v.v.\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state  # [1, seq_len, hidden_dim]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "        \n",
        "        mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "        pooled = torch.sum(embeddings * mask, dim=1) / torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "        \n",
        "        # L2 normalize\n",
        "        pooled = F.normalize(pooled, p=2, dim=1)\n",
        "        return pooled[0].cpu().numpy().tolist()\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(\"Error in get_phobert_embedding:\", e, \"for text snippet:\", cleaned_text[:100])\n",
        "        # Trả về vector zero thay vì crash\n",
        "        return [0.0] * 768\n",
        "\n",
        "\n",
        "def ingest_urls_to_mongo(urls, connection_string, db_name, collection_name):\n",
        "    \"\"\"\n",
        "    Tải nội dung từ các URL, làm sạch bằng BeautifulSoup, chia nhỏ (chunk),\n",
        "    tính embedding bằng PhoBERT (521 chiều) và lưu vào MongoDB.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "    client = MongoClient(connection_string)\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            loader = UnstructuredURLLoader(urls=[url])\n",
        "            docs = loader.load()\n",
        "            if not docs:\n",
        "                print(f\"Không load được nội dung từ {url}\")\n",
        "                continue\n",
        "\n",
        "            for doc in docs:\n",
        "                # Làm sạch HTML để lấy nội dung text thuần\n",
        "                content = BeautifulSoup(doc.page_content, \"html.parser\").get_text(separator=\" \", strip=True)\n",
        "                content = content.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
        "                chunks = text_splitter.split_text(content)\n",
        "                if not chunks:\n",
        "                    print(f\"Không có chunk nào ở {url}\")\n",
        "                    continue\n",
        "\n",
        "                for idx, chunk in enumerate(chunks):\n",
        "                    if not chunk.strip():\n",
        "                        print(f\"Bỏ qua chunk rỗng (chunk {idx}) ở {url}\")\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        embedding = get_phobert_embedding(chunk)\n",
        "                        doc_dict = {\n",
        "                            \"url\": url,\n",
        "                            \"chunk_index\": idx,\n",
        "                            \"page_content\": chunk,\n",
        "                            \"metadata\": doc.metadata,\n",
        "                            \"embedding\": embedding\n",
        "                        }\n",
        "                        collection.insert_one(doc_dict)\n",
        "                        print(f\"Đã chèn chunk {idx} từ {url}\")\n",
        "                    except Exception as embed_err:\n",
        "                        print(f\"Lỗi tạo embedding chunk {idx} từ {url}: {embed_err}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi khi ingest {url}: {e}\")\n",
        "\n",
        "    client.close()\n",
        "    print(\"Hoàn thành ingest dữ liệu vào MongoDB.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "c3W12ll6j3-i"
      },
      "outputs": [],
      "source": [
        "# Danh sách các URL cần ingest\n",
        "urls = [\n",
        "    \"https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/u-hac-lanh-tinh-cua-da-vi\",\n",
        "    \"https://www.fvhospital.com/tin-suc-khoe/ung-thu-sac-to-la-gi-hieu-dung-ve-ung-thu-hac-to-va-benh-ly-sac-to-da/\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/ung-thu-bieu-mo-te-bao-day-chan-doan-va-dieu-tri-vi#:~:text=Ung%20th%C6%B0%20bi%E1%BB%83u%20m%C3%B4%20t%E1%BA%BF%20b%C3%A0o%20%C4%91%C3%A1y%20(Basal%20cell%20carcinoma,s%C3%A2u%20nh%E1%BA%A5t%20c%E1%BB%A7a%20bi%E1%BB%83u%20m%C3%B4.\",\n",
        "    \"https://www.vinmec.com/vie/benh/ung-thu-bieu-mo-te-bao-day-4315\",\n",
        "    \"https://tamanhhospital.vn/ung-thu-bieu-mo-te-bao-day/\",\n",
        "    \"https://suckhoedoisong.vn/ung-thu-bieu-mo-te-bao-day-tai-phat-va-phuong-phap-chua-hieu-qua-16923011211073782.htm\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/dau-hieu-da-bi-day-sung-anh-nang-vi\",\n",
        "    \"https://www.nhathuocankhang.com/benh/benh-day-sung-quang-hoa\",\n",
        "    \"https://nhathuoclongchau.com.vn/benh/day-sung-anh-sang-520.html\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/nhung-dieu-can-biet-ve-chung-day-sung-tiet-ba-vi\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/day-sung-da-dau-nguyen-nhan-va-trieu-chung-vi\",\n",
        "    \"https://nhathuoclongchau.com.vn/bai-viet/benh-day-sung-da-dau-nguyen-nhan-trieu-chung-va-cach-dieu-tri.html\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/tim-hieu-ve-u-xo-kinh-mun-thit-vi\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/noi-u-va-buou-bieu-hien-dieu-gi-tren-da-cua-ban-vi\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/tim-hieu-not-ruoi-la-gi-va-cac-loai-not-ruoi-vi\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/not-ruoi-hinh-thanh-nhu-the-nao-vi\",\n",
        "    \"https://tamanhhospital.vn/not-ruoi/\",\n",
        "    \"https://medlatec.vn/tin-tuc/vet-thuong-mach-mau-la-gi-nguyen-tac-va-ky-thuat-so-cuu-dung-cach\",\n",
        "    \"https://nhathuoclongchau.com.vn/bai-viet/vo-mach-mau-duoi-da-nguyen-nhan-va-cach-dieu-tri-72683.html\",\n",
        "    \"https://www.vinmec.com/vie/bai-viet/viem-mach-ngoai-da-nhung-dieu-can-biet-vi\",\n",
        "    \"https://hellobacsi.com/suc-khoe/trieu-chung/vo-mach-mau-duoi-da/\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_string = (\n",
        "        \"mongodb+srv://nguyendoantienanh2302:HTwtQDQKssuDlLzb@rag4llm.hsreq.mongodb.net/?retryWrites=true&w=majority&appName=RAG4LLM\"\n",
        "    )\n",
        "\n",
        "db_name = \"rag_database\"\n",
        "collection_name = \"documents\"\n",
        "\n",
        "client = MongoClient(connection_string)\n",
        "db = client[db_name]\n",
        "collection = db[collection_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "U6PkQxm2mSWz",
        "outputId": "1fdd93b8-a743-4312-e0dd-ce970a477ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã chèn chunk 0 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 1 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 2 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 3 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 4 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 5 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 6 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 7 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 8 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 9 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 10 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 11 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 12 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 13 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 14 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 15 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 16 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 17 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 18 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 19 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 20 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 21 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 22 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 23 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 24 từ https://www.vinmec.com/vie/benh/ung-thu-hac-to-da-4723\n",
            "Đã chèn chunk 0 từ https://www.vinmec.com/vie/bai-viet/u-hac-lanh-tinh-cua-da-vi\n",
            "Đã chèn chunk 1 từ https://www.vinmec.com/vie/bai-viet/u-hac-lanh-tinh-cua-da-vi\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mingest_urls_to_mongo\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mingest_urls_to_mongo\u001b[39m\u001b[34m(urls, connection_string, db_name, collection_name)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     embedding = \u001b[43mget_phobert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     doc_dict = {\n\u001b[32m     84\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: url,\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m: idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m: embedding\n\u001b[32m     89\u001b[39m     }\n\u001b[32m     90\u001b[39m     collection.insert_one(doc_dict)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mget_phobert_embedding\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 3) Lấy output, mean pooling, v.v.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m embeddings = outputs.last_hidden_state  \u001b[38;5;66;03m# [1, seq_len, hidden_dim]\u001b[39;00m\n\u001b[32m     34\u001b[39m attention_mask = inputs[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    974\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    989\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    620\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    621\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    622\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         output_attentions,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    509\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    510\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    517\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    518\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    519\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    529\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:447\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    439\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    446\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    457\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370\u001b[39m, in \u001b[36mRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    366\u001b[39m is_causal = (\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    368\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    380\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "ingest_urls_to_mongo(urls, connection_string, db_name, collection_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_reflection_ollama(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Sử dụng ChatOllama để tinh chỉnh query.\n",
        "    Nếu chưa cấu hình Ollama, có thể trả về query gốc.\n",
        "    \"\"\"\n",
        "    # Nếu bạn có endpoint thực, uncomment và điều chỉnh phần dưới:\n",
        "    \"\"\"\n",
        "    prompt = f\"Refine the following medical query to be more specific and clear:\\nQuery: {query}\"\n",
        "    url = \"http://localhost:11434/generate\"\n",
        "    payload = {\"prompt\": prompt, \"model\": \"llama2\", \"temperature\": 0.0}\n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        refined = data.get(\"generated_text\", \"\").strip()\n",
        "        return refined if refined else query\n",
        "    except Exception as e:\n",
        "        print(\"Lỗi Self Reflection:\", e)\n",
        "        return query\n",
        "    \"\"\"\n",
        "    # Placeholder: trả về query gốc\n",
        "    return query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã tính embedding cho 20 câu y học.\n"
          ]
        }
      ],
      "source": [
        "# 20 câu liên quan đến chủ đề y học (giả lập)\n",
        "medical_queries = [\n",
        "    \"Ung thư hắc tố da là gì?\",\n",
        "    \"Triệu chứng ung thư biểu mô tế bào đáy\",\n",
        "    \"Cách điều trị u hắc lành tính\",\n",
        "    \"Dấu hiệu da bị tổn thương do tia UV\",\n",
        "    \"Nguyên nhân gây ung thư hắc tố da\",\n",
        "    \"Phòng ngừa ung thư da hiệu quả\",\n",
        "    \"Tác động của tia UV đến sức khỏe da\",\n",
        "    \"Biểu hiện của ung thư da sớm\",\n",
        "    \"Phương pháp chẩn đoán ung thư hắc tố da\",\n",
        "    \"Cách điều trị ung thư biểu mô tế bào đáy\",\n",
        "    \"Phẫu thuật ung thư da\",\n",
        "    \"Liệu pháp điều trị ung thư da\",\n",
        "    \"Tầm quan trọng của kem chống nắng\",\n",
        "    \"Rủi ro khi tiếp xúc nhiều với tia UV\",\n",
        "    \"Cách chăm sóc da sau điều trị ung thư\",\n",
        "    \"Các bước chẩn đoán bệnh da liễu\",\n",
        "    \"Phòng tránh các bệnh về da\",\n",
        "    \"Tầm quan trọng của kiểm tra da định kỳ\",\n",
        "    \"Các dấu hiệu cần gặp bác sĩ da liễu\",\n",
        "    \"Phương pháp điều trị bệnh da hiện đại\"\n",
        "]\n",
        "\n",
        "# Tính embedding cho 20 câu y học\n",
        "medical_queries_embeddings = []\n",
        "for q in medical_queries:\n",
        "    emb = get_phobert_embedding(q)\n",
        "    medical_queries_embeddings.append((q, emb))\n",
        "print(\"Đã tính embedding cho 20 câu y học.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_cosine_similarity(vec1, vec2):\n",
        "    v1 = np.array(vec1, dtype=float)\n",
        "    v2 = np.array(vec2, dtype=float)\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm1 = np.linalg.norm(v1)\n",
        "    norm2 = np.linalg.norm(v2)\n",
        "    if norm1 * norm2 == 0:\n",
        "        return 0.0\n",
        "    return dot / (norm1 * norm2)\n",
        "\n",
        "def check_need_rag(query_emb, threshold=0.9):\n",
        "    \"\"\"\n",
        "    So sánh query_emb với 20 câu y học.\n",
        "    Nếu bất kỳ similarity nào >= threshold, trả về True.\n",
        "    \"\"\"\n",
        "    best_sim = 0\n",
        "    for (q_text, q_emb) in medical_queries_embeddings:\n",
        "        sim = calc_cosine_similarity(query_emb, q_emb)\n",
        "        if sim > best_sim:\n",
        "            best_sim = sim\n",
        "    print(f\"[check_need_rag] Best similarity = {best_sim:.3f}\")\n",
        "    return best_sim >= threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_query_results(query, limit=10):\n",
        "    query_embedding = get_phobert_embedding(query)\n",
        "    pipeline_vector = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embedding\",\n",
        "                \"exact\": True,\n",
        "                \"limit\": limit\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,\n",
        "                \"url\": 1,\n",
        "                \"page_content\": 1,\n",
        "                \"score\": {\"$meta\": \"searchScore\"}\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    results_cursor = collection.aggregate(pipeline_vector)  # Dùng biến collection\n",
        "    results = [doc for doc in results_cursor]\n",
        "    return results\n",
        "\n",
        "def bm25_rerank(query: str, docs: list, top_n=5):\n",
        "    corpus = [doc[\"page_content\"] for doc in docs]\n",
        "    tokenized_corpus = [c.split() for c in corpus]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    query_tokens = query.split()\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    scored_docs = []\n",
        "    for doc, s in zip(docs, scores):\n",
        "        doc[\"bm25_score\"] = float(s)\n",
        "        scored_docs.append(doc)\n",
        "    scored_docs = sorted(scored_docs, key=lambda x: x[\"bm25_score\"], reverse=True)\n",
        "    return scored_docs[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# from typing import List\n",
        "# from langchain_core.tools import tool\n",
        "# from langchain_ollama import ChatOllama\n",
        "\n",
        "# # Tool cho model Classification\n",
        "# @tool\n",
        "# def classify_text(text: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Phân loại văn bản thành các nhãn cụ thể.\n",
        "#     Ví dụ: \"Tin tức\", \"Giải trí\", \"Thể thao\", v.v.\n",
        "#     \"\"\"\n",
        "#     # Ở đây bạn có thể gọi model classification thực tế của mình.\n",
        "#     # Ví dụ: result = classification_model.predict(text)\n",
        "#     # Giả lập kết quả:\n",
        "#     result = \"Classification result: Category A\"\n",
        "#     return result\n",
        "\n",
        "# # Tool cho model Segmentation\n",
        "# @tool\n",
        "# def segment_image(image_path: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Phân đoạn hình ảnh và trả về kết quả phân đoạn.\n",
        "#     Ví dụ: xác định các vùng trong ảnh, như \"foreground\", \"background\", ...\n",
        "#     \"\"\"\n",
        "#     # Ở đây bạn có thể gọi model segmentation thực tế của mình.\n",
        "#     # Ví dụ: mask = segmentation_model.predict(image_path)\n",
        "#     # Giả lập kết quả:\n",
        "#     result = \"Segmentation result: Detected objects and regions\"\n",
        "#     return result\n",
        "\n",
        "# # Khởi tạo ChatOllama (ví dụ sử dụng model \"llama2\")\n",
        "# llm = ChatOllama(\n",
        "#     model=\"llama2\",  # hoặc model khác theo yêu cầu\n",
        "#     temperature=0,\n",
        "#     streaming=False  # Tắt streaming để tránh lỗi định dạng message\n",
        "# )\n",
        "\n",
        "# # Bind các tool vào model LLM\n",
        "# llm = llm.bind_tools([classify_text, segment_image])\n",
        "llm = ChatOllama(\n",
        "    model=\"phi3\",      # Tên model bạn đang load\n",
        "    base_url=\"http://localhost:11434\",  # Nếu Ollama chạy cổng 11434\n",
        "    temperature=0.7,\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "def llm_answer_with_ollama(prompt: str) -> str:\n",
        "    try:\n",
        "        # Sử dụng phương thức predict() để gọi model, tránh phải định dạng message thủ công\n",
        "        response = llm.predict(prompt)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Lỗi khi gọi llm.predict:\", e)\n",
        "        return \"[Lỗi] Không thể gọi model.\"\n",
        "\n",
        "\n",
        "def normal_model_answer(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Sử dụng ChatOllama cho câu trả lời khi không cần RAG.\n",
        "    \"\"\"\n",
        "    prompt = f\"Trả lời câu hỏi sau một cách ngắn gọn và rõ ràng:\\nQuery: {query}\"\n",
        "    return llm_answer_with_ollama(prompt)\n",
        "\n",
        "def rag_llm_answer(query: str, docs: list) -> str:\n",
        "    \"\"\"\n",
        "    Sử dụng ChatOllama để trả lời dựa trên query và các tài liệu liên quan.\n",
        "    \"\"\"\n",
        "    combined_text = \"\\n\\n\".join([f\"URL: {doc['url']}\\nContent: {doc['page_content']}\" for doc in docs])\n",
        "    prompt = (f\"Bạn là chuyên gia y tế hàng đầu trong lĩnh vực da liễu hãy giúp tôi trả lời những câu hỏi liên quan.\"\n",
        "              f\"Câu hỏi: {query}\\n\\n\"\n",
        "              f\"Tài liệu:\\n{combined_text}\\n\\n\"\n",
        "              f\"Trả lời một ách ngắn gọn dễ hiểu từ tài liệu được cung cấp\")\n",
        "    return llm_answer_with_ollama(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 7. Ghép Pipeline\n",
        "# ---------------------------\n",
        "def pipeline(query_text: str):\n",
        "    # B1: Self Reflection (với Ollama)\n",
        "    refined_query = self_reflection_ollama(query_text)\n",
        "    print(f\"[Self Reflection] refined_query = {refined_query}\")\n",
        "    \n",
        "    # B2: Tính embedding cho query\n",
        "    query_emb = get_phobert_embedding(refined_query)\n",
        "    \n",
        "    # B3: Kiểm tra xem cần dùng RAG không (so sánh với 20 câu y học)\n",
        "    need_rag = check_need_rag(query_emb, threshold=0.9)\n",
        "    if not need_rag:\n",
        "        print(\"[INFO] Similarity < 0.9 => Dùng model bình thường\")\n",
        "        return normal_model_answer(refined_query)\n",
        "    else:\n",
        "        print(\"[INFO] Similarity >= 0.9 => Dùng RAG pipeline\")\n",
        "        vec_docs = get_query_results(refined_query, limit=10)\n",
        "        if not vec_docs:\n",
        "            return \"[RAG] Không tìm thấy tài liệu phù hợp.\"\n",
        "        top_docs = bm25_rerank(refined_query, vec_docs, top_n=5)\n",
        "        if not top_docs:\n",
        "            return \"[RAG] Không có tài liệu sau BM25.\"\n",
        "        answer = rag_llm_answer(refined_query, top_docs)\n",
        "        return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from openai==0.28) (3.11.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\msi\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.66.5\n",
            "    Uninstalling openai-1.66.5:\n",
            "      Successfully uninstalled openai-1.66.5\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"YOUR_API_KEY_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "def llm_answer_with_openai(query: str, docs: list = None) -> str:\n",
        "    \"\"\"\n",
        "    Nếu có danh sách tài liệu docs, xây dựng prompt theo RAG.\n",
        "    Nếu không có, xây dựng prompt đơn giản chỉ chứa query.\n",
        "    \"\"\"\n",
        "    if docs:\n",
        "        combined_text = \"\\n\\n\".join([f\"URL: {doc['url']}\\nContent: {doc['page_content']}\" for doc in docs])\n",
        "        prompt = (\n",
        "            f\"Bạn là chuyên gia y tế hàng đầu trong lĩnh vực da liễu. Hãy trả lời câu hỏi dưới đây dựa trên các tài liệu được cung cấp.\\n\\n\"\n",
        "            f\"Câu hỏi: {query}\\n\\n\"\n",
        "            f\"Tài liệu:\\n{combined_text}\\n\\n\"\n",
        "            f\"Trả lời một cách ngắn gọn và dễ hiểu.\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = f\"Trả lời câu hỏi sau một cách ngắn gọn và rõ ràng:\\nQuery: {query}\"\n",
        "\n",
        "    try:\n",
        "        # Gọi API với giao diện mới của openai (sau migrate)\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Hoặc model khác mà bạn cần\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        # Truy cập nội dung câu trả lời theo giao diện mới\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        print(\"Lỗi khi gọi OpenAI:\", e)\n",
        "        return \"[Lỗi] Không thể gọi model OpenAI.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 7. Ghép Pipeline\n",
        "# ---------------------------\n",
        "def pipeline1(query_text: str):\n",
        "    # B1: Self Reflection (với Ollama)\n",
        "    # refined_query = self_reflection_ollama(query_text)\n",
        "    # print(f\"[Self Reflection] refined_query = {refined_query}\")\n",
        "    \n",
        "    # B2: Tính embedding cho query\n",
        "    query_emb = get_phobert_embedding(query_text)\n",
        "    \n",
        "    # B3: Kiểm tra xem cần dùng RAG không (so sánh với 20 câu y học)\n",
        "    need_rag = check_need_rag(query_emb, threshold=0.9)\n",
        "    if not need_rag:\n",
        "        print(\"[INFO] Similarity < 0.9 => Dùng model bình thường\")\n",
        "        return llm_answer_with_openai(query_text)\n",
        "    else:\n",
        "        print(\"[INFO] Similarity >= 0.9 => Dùng RAG pipeline\")\n",
        "        vec_docs = get_query_results(query_text, limit=10)\n",
        "        if not vec_docs:\n",
        "            return \"[RAG] Không tìm thấy tài liệu phù hợp.\"\n",
        "        top_docs = bm25_rerank(query_text, vec_docs, top_n=5)\n",
        "        if not top_docs:\n",
        "            return \"[RAG] Không có tài liệu sau BM25.\"\n",
        "        answer = llm_answer_with_openai(query_text, top_docs)\n",
        "        return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[check_need_rag] Best similarity = 0.906\n",
            "[INFO] Similarity >= 0.9 => Dùng RAG pipeline\n",
            "Lỗi khi gọi OpenAI: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "=== KẾT QUẢ CUỐI ===\n",
            "[Lỗi] Không thể gọi model OpenAI.\n"
          ]
        }
      ],
      "source": [
        "query_text = (\"Cách điều trị ung thư hắc tố da\")\n",
        "result = pipeline1(query_text)\n",
        "print(\"=== KẾT QUẢ CUỐI ===\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xjGczJj-sces"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Self Reflection] refined_query = Cách điều trị ung thư hắc tố da\n",
            "[check_need_rag] Best similarity = 0.906\n",
            "[INFO] Similarity >= 0.9 => Dùng RAG pipeline\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 8. Chạy thử pipeline với query mẫu\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m query_text = (\u001b[33m\"\u001b[39m\u001b[33mCách điều trị ung thư hắc tố da\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== KẾT QUẢ CUỐI ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(query_text)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m top_docs:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m[RAG] Không có tài liệu sau BM25.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m answer = \u001b[43mrag_llm_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefined_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mrag_llm_answer\u001b[39m\u001b[34m(query, docs)\u001b[39m\n\u001b[32m     73\u001b[39m combined_text = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[33m'\u001b[39m\u001b[33mpage_content\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[32m     74\u001b[39m prompt = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBạn là chuyên gia y tế hàng đầu trong lĩnh vực da liễu hãy giúp tôi trả lời những câu hỏi liên quan.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTài liệu:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcombined_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrả lời:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm_answer_with_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mllm_answer_with_ollama\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_answer_with_ollama\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m# Sử dụng phương thức predict() để gọi model, tránh phải định dạng message thủ công\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1111\u001b[39m, in \u001b[36mBaseChatModel.predict\u001b[39m\u001b[34m(self, text, stop, **kwargs)\u001b[39m\n\u001b[32m   1106\u001b[39m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m0.1.7\u001b[39m\u001b[33m\"\u001b[39m, alternative=\u001b[33m\"\u001b[39m\u001b[33minvoke\u001b[39m\u001b[33m\"\u001b[39m, removal=\u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, *, stop: Optional[Sequence[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m   1109\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1110\u001b[39m     _stop = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result.content, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result.content\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1074\u001b[39m, in \u001b[36mBaseChatModel.__call__\u001b[39m\u001b[34m(self, messages, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m0.1.7\u001b[39m\u001b[33m\"\u001b[39m, alternative=\u001b[33m\"\u001b[39m\u001b[33minvoke\u001b[39m\u001b[33m\"\u001b[39m, removal=\u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1072\u001b[39m     **kwargs: Any,\n\u001b[32m   1073\u001b[39m ) -> BaseMessage:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m     generation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation.message\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_ollama\\chat_models.py:705\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    699\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    700\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    703\u001b[39m     **kwargs: Any,\n\u001b[32m    704\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    709\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    710\u001b[39m         message=AIMessage(\n\u001b[32m    711\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    716\u001b[39m         generation_info=generation_info,\n\u001b[32m    717\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_ollama\\chat_models.py:642\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    634\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    635\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     **kwargs: Any,\n\u001b[32m    640\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    641\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_ollama\\chat_models.py:727\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    722\u001b[39m     messages: List[BaseMessage],\n\u001b[32m    723\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    724\u001b[39m     **kwargs: Any,\n\u001b[32m    725\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    726\u001b[39m     is_thinking = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    744\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langchain_ollama\\chat_models.py:629\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\ollama\\_client.py:170\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    167\u001b[39m   e.response.read()\n\u001b[32m    168\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpx\\_models.py:929\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m decoder = LineDecoder()\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpx\\_models.py:916\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    914\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpx\\_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpx\\_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpx\\_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpx\\_transports\\default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\nlp\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# 8. Chạy thử pipeline với query mẫu\n",
        "# ---------------------------\n",
        "query_text = (\"Cách điều trị ung thư hắc tố da\")\n",
        "result = pipeline(query_text)\n",
        "print(\"=== KẾT QUẢ CUỐI ===\")\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
